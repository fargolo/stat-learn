xlim(0,8)+ylim(0,8)+
geom_abline(slope=1,intercept=0)
mark_ii_preds <- mark_ii(x = x_features,y = y_target,
eta=0.001,reps = 60)
mark_ii_preds <- mark_ii(x = x_features,y = y_target,
eta=0.0001,reps = 60)
acc_data <- data.frame(y_preds=mark_ii_preds,
y_targs=y_target)
acc_data$errors <- y_target - mark_ii_preds
ggplot(acc_data,aes(y=y_preds,x=y_targs,color=errors))+
geom_point()+xlim(0,10)+ylim(0,10)+
geom_abline(slope = 1,intercept = 0)
iris$mark <- mark_ii_preds
ggplot(iris, aes(y=mark,x=Petal.Length,
color=Species))+
geom_point()+
xlim(0,8)+ylim(0,8)+
geom_abline(slope=1,intercept=0)
cor(mark_ii_preds,y_target)
mark_ii_preds <- mark_ii(x = x_features,y = y_target,
eta=0.00085,reps = 60)
mark_ii_preds <- mark_ii(x = x_features,y = y_target,
eta=0.00075,reps = 60)
acc_data <- data.frame(y_preds=mark_ii_preds,
y_targs=y_target)
acc_data$errors <- y_target - mark_ii_preds
ggplot(acc_data,aes(y=y_preds,x=y_targs,color=errors))+
geom_point()+xlim(0,10)+ylim(0,10)+
geom_abline(slope = 1,intercept = 0)
iris$mark <- mark_ii_preds
ggplot(iris, aes(y=mark,x=Petal.Length,
color=Species))+
geom_point()+
xlim(0,8)+ylim(0,8)+
geom_abline(slope=1,intercept=0)
cor(mark_ii_preds,y_target)
mark_ii_preds <- mark_ii(x = x_features,y = y_target,
eta=0.00075,reps = 60)
acc_data <- data.frame(y_preds=mark_ii_preds,
y_targs=y_target)
acc_data$errors <- y_target - mark_ii_preds
ggplot(acc_data,aes(y=y_preds,x=y_targs,color=errors))+
geom_point()+xlim(0,10)+ylim(0,10)+
geom_abline(slope = 1,intercept = 0)
iris$mark <- mark_ii_preds
ggplot(iris, aes(y=mark,x=Petal.Length,
color=Species))+
geom_point()+
xlim(0,8)+ylim(0,8)+
geom_abline(slope=1,intercept=0)
cor(mark_ii_preds,y_target)
mark_ii_preds <- mark_ii(x = x_features,y = y_target,
eta=0.00075,reps = 60)
acc_data <- data.frame(y_preds=mark_ii_preds,
y_targs=y_target)
acc_data$errors <- y_target - mark_ii_preds
ggplot(acc_data,aes(y=y_preds,x=y_targs,color=errors))+
geom_point()+xlim(0,10)+ylim(0,10)+
geom_abline(slope = 1,intercept = 0)
iris$mark <- mark_ii_preds
ggplot(iris, aes(y=mark,x=Petal.Length,
color=Species))+
geom_point()+
xlim(0,8)+ylim(0,8)+
geom_abline(slope=1,intercept=0)
cor(mark_ii_preds,y_target)
mark_ii_preds <- mark_ii(x = x_features,y = y_target,
eta=0.00075,reps = 60)
acc_data <- data.frame(y_preds=mark_ii_preds,
y_targs=y_target)
acc_data$errors <- y_target - mark_ii_preds
ggplot(acc_data,aes(y=y_preds,x=y_targs,color=errors))+
geom_point()+xlim(0,10)+ylim(0,10)+
geom_abline(slope = 1,intercept = 0)
iris$mark <- mark_ii_preds
ggplot(iris, aes(y=mark,x=Petal.Length,
color=Species))+
geom_point()+
xlim(0,8)+ylim(0,8)+
geom_abline(slope=1,intercept=0)
cor(mark_ii_preds,y_target)
mark_ii_preds <- mark_ii(x = x_features,y = y_target,
eta=0.00075,reps = 60)
acc_data <- data.frame(y_preds=mark_ii_preds,
y_targs=y_target)
acc_data$errors <- y_target - mark_ii_preds
ggplot(acc_data,aes(y=y_preds,x=y_targs,color=errors))+
geom_point()+xlim(0,10)+ylim(0,10)+
geom_abline(slope = 1,intercept = 0)
iris$mark <- mark_ii_preds
ggplot(iris, aes(y=mark,x=Petal.Length,
color=Species))+
geom_point()+
xlim(0,8)+ylim(0,8)+
geom_abline(slope=1,intercept=0)
cor(mark_ii_preds,y_target)
mark_ii_preds <- mark_ii(x = x_features,y = y_target,
eta=0.00075,reps = 60)
acc_data <- data.frame(y_preds=mark_ii_preds,
y_targs=y_target)
acc_data$errors <- y_target - mark_ii_preds
ggplot(acc_data,aes(y=y_preds,x=y_targs,color=errors))+
geom_point()+xlim(0,10)+ylim(0,10)+
geom_abline(slope = 1,intercept = 0)
iris$mark <- mark_ii_preds
ggplot(iris, aes(y=mark,x=Petal.Length,
color=Species))+
geom_point()+
xlim(0,8)+ylim(0,8)+
geom_abline(slope=1,intercept=0)
cor(mark_ii_preds,y_target)
library(png)
book_cov <- readPNG("../images/cover.png")
?rasterGrob
library(grid)
book_img <- rasterGrob(book_cov,interpolate = T)
library(ggplot2)
ggplot()+annotation_custom(book_img,xmin = -Inf,xmax = Inf,ymin=-Inf,ymax=Inf)
ggplot()+annotation_custom(book_img,xmin = -Inf,xmax = Inf,ymin=-Inf,ymax=Inf)
ggplot()+annotation_custom(book_img,xmin = -Inf,xmax = Inf,ymin=-Inf,ymax=Inf)
library(magrittr)
library(ggplot2)
set.seed(2600)
mark_ii <- function(x, y, eta, reps=1) {
# inicializa pesos randomicos de distribuicao normal
w1 <- rnorm(n = (dim(x)[2]+1)) %>% as.matrix # numero de pesos = numero de colunas em x + bias
w21 <- rnorm(2) %>% as.matrix
w22 <- rnorm(2) %>% as.matrix
ypreds <- rep(0,dim(x)[1]) # inicializa predicoes em 0
yerrors <- rep(0,dim(x)[1]) # inicializa predicoes em 0
for (j in 1:reps){
print(paste("This is training epoch:",j))
print(paste("Current weights:",w1,w21,w21))
# Processa as observacoes em x de forma aleatoria
for (i in sample(1:length(y),replace=F)) {
# predicao
ypred1 <- sum(w1 %*% c(as.numeric(x[i, ]),1))
ypred21 <- sum(w21 %*% as.numeric(ypred1))
ypred22 <- sum(w22 %*% as.numeric(ypred1))
out <- sum(ypred21,ypred22)
# update em w . Eta ja ajustado para 1/2*eta
delta_w22 <- eta * (-1) * (y[i] - (ypred21 + ypred22)) * ypred1 #d/dw22(targ - out)^2
delta_w21 <- eta * (-1) * (y[i] - (ypred21 + ypred22)) * ypred1 #d/dw21(targ - out)^2
#d/dw1(targ - out)^2 = 2(y[i] - (ypred21 + ypred22)) (-1) (sum(w21*w1*x[i,]) + sum(w21*w1*x[i,]))'
# d/dw1 (sum(w21*w1*x[i,]) + sum(w21*w1*x[i,])) =  (sum(w21*x[i,]) + sum(w21*x[i,]))
delta_w1 <- eta * (y[i] - (ypred21 + ypred22)) * -1 *
(sum(w21 %*% c(as.numeric(x[i,]),1)) + sum(w22 %*% c(as.numeric(x[i,]),1)))
#nota: x[i,] sera multiplicado como matriz (dot product)
w1 <- w1 - delta_w1
w21 <- w21 - delta_w21
w22 <- w22 - delta_w22
ypreds[i] <- out # salva predicao21 atual
yerrors[i] <- ypreds[i] - y[i]
}
print(paste("Mean squared error:", mean((yerrors)^2)))
}
return(ypreds)
}
mark_ii <- function(x, y, eta, reps=1) {
# inicializa pesos randomicos de distribuicao normal
w1 <- rnorm(n = (dim(x)[2]+1)) %>% as.matrix # numero de pesos = numero de colunas em x + bias
w21 <- rnorm(2) %>% as.matrix
w22 <- rnorm(2) %>% as.matrix
ypreds <- rep(0,dim(x)[1]) # inicializa predicoes em 0
yerrors <- rep(0,dim(x)[1]) # inicializa predicoes em 0
for (j in 1:reps){
print(paste("This is training epoch:",j))
print(paste("Current weights:",w1,w21,w21))
# Processa as observacoes em x de forma aleatoria
for (i in sample(1:length(y),replace=F)) {
# predicao
ypred1 <- sum(w1 %*% c(as.numeric(x[i, ]),1))
ypred21 <- sum(w21 %*% as.numeric(ypred1))
ypred22 <- sum(w22 %*% as.numeric(ypred1))
out <- sum(ypred21,ypred22)
# update em w . Eta ja ajustado para 1/2*eta
delta_w22 <- eta * (-1) * (y[i] - (ypred21 + ypred22)) * ypred1 #d/dw22(targ - out)^2
delta_w21 <- eta * (-1) * (y[i] - (ypred21 + ypred22)) * ypred1 #d/dw21(targ - out)^2
#d/dw1(targ - out)^2 = 2(y[i] - (ypred21 + ypred22)) (-1) (sum(w21*w1*x[i,]) + sum(w21*w1*x[i,]))'
# d/dw1 (sum(w21*w1*x[i,]) + sum(w21*w1*x[i,])) =  (sum(w21*x[i,]) + sum(w21*x[i,]))
delta_w1 <- eta * (y[i] - (ypred21 + ypred22)) * -1 *
(sum(w21 %*% c(as.numeric(x[i,]),1)) + sum(w22 %*% c(as.numeric(x[i,]),1)))
#nota: x[i,] sera multiplicado como matriz (dot product)
w1 <- w1 - delta_w1
w21 <- w21 - delta_w21
w22 <- w22 - delta_w22
ypreds[i] <- out # salva predicao21 atual
yerrors[i] <- ypreds[i] - y[i]
}
print(paste("Mean squared error:", mean((yerrors)^2)))
}
return(ypreds)
}
train_df <- iris[, c(1, 2, 3)]
names(train_df) <- c("s.len", "s.wid", "p.len")
head(train_df)
train_df[60:65,]
x_features <- train_df[, c(1, 2)]
y_target <- train_df[, 3]
mark_ii_preds <- mark_ii(x = x_features,y = y_target,
eta=0.000001,reps = 40)
mark_ii_preds
acc_data <- data.frame(y_preds=mark_ii_preds,
y_targs=y_target)
acc_data$errors <- y_target - mark_ii_preds
cor.test(acc_data$y_preds,acc_data$y_targs)
ggplot(acc_data,aes(y=y_preds,x=y_targs,color=errors))+
geom_point()+xlim(0,10)+ylim(0,10)+xlab("Observações")+ylab("Predições")+
scale_color_continuous(low="plum2",high="purple3",name="MSE")+
geom_abline(slope = 1,intercept = 0,color="gold")+
theme_hc(style="darkunica")
library(ggthemes)
ggplot(acc_data,aes(y=y_preds,x=y_targs,color=errors))+
geom_point()+xlim(0,10)+ylim(0,10)+xlab("Observações")+ylab("Predições")+
scale_color_continuous(low="plum2",high="purple3",name="MSE")+
geom_abline(slope = 1,intercept = 0,color="gold")+
theme_hc(style="darkunica")
library(caret)
class_df$time_sc <- scale(class_df$time)
library(magrittr)
library(ggplot2)
set.seed(2600)
mark_ii <- function(x, y, eta, reps=1) {
# inicializa pesos randomicos de distribuicao normal
w1 <- rnorm(n = (dim(x)[2]+1)) %>% as.matrix # numero de pesos = numero de colunas em x + bias
w21 <- rnorm(2) %>% as.matrix
w22 <- rnorm(2) %>% as.matrix
ypreds <- rep(0,dim(x)[1]) # inicializa predicoes em 0
yerrors <- rep(0,dim(x)[1]) # inicializa predicoes em 0
for (j in 1:reps){
print(paste("This is training epoch:",j))
print(paste("Current weights:",w1,w21,w21))
# Processa as observacoes em x de forma aleatoria
for (i in sample(1:length(y),replace=F)) {
# predicao
ypred1 <- sum(w1 %*% c(as.numeric(x[i, ]),1))
ypred21 <- sum(w21 %*% as.numeric(ypred1))
ypred22 <- sum(w22 %*% as.numeric(ypred1))
out <- sum(ypred21,ypred22)
# update em w . Eta ja ajustado para 1/2*eta
delta_w22 <- eta * (-1) * (y[i] - (ypred21 + ypred22)) * ypred1 #d/dw22(targ - out)^2
delta_w21 <- eta * (-1) * (y[i] - (ypred21 + ypred22)) * ypred1 #d/dw21(targ - out)^2
#d/dw1(targ - out)^2 = 2(y[i] - (ypred21 + ypred22)) (-1) (sum(w21*w1*x[i,]) + sum(w21*w1*x[i,]))'
# d/dw1 (sum(w21*w1*x[i,]) + sum(w21*w1*x[i,])) =  (sum(w21*x[i,]) + sum(w21*x[i,]))
delta_w1 <- eta * (y[i] - (ypred21 + ypred22)) * -1 *
(sum(w21 %*% c(as.numeric(x[i,]),1)) + sum(w22 %*% c(as.numeric(x[i,]),1)))
#nota: x[i,] sera multiplicado como matriz (dot product)
w1 <- w1 - delta_w1
w21 <- w21 - delta_w21
w22 <- w22 - delta_w22
ypreds[i] <- out # salva predicao21 atual
yerrors[i] <- ypreds[i] - y[i]
}
print(paste("Mean squared error:", mean((yerrors)^2)))
}
return(ypreds)
}
train_df <- iris[, c(1, 2, 3)]
names(train_df) <- c("s.len", "s.wid", "p.len")
head(train_df)
train_df[60:65,]
iris
x_features <- train_df[, c(1, 2)]
y_target <- train_df[, 3]
x_features <- train_df[, c(1, 2)]
y_target <- train_df[, 3]
# Convergencia boa
mark_ii_preds <- mark_ii(x = x_features,y = y_target,
eta=0.000001,reps = 40)
acc_data$errors <- y_target - mark_ii_preds
cor.test(acc_data$y_preds,acc_data$y_targs)
ggplot(acc_data,aes(y=y_preds,x=y_targs,color=errors))+
geom_point()+xlim(0,10)+ylim(0,10)+xlab("Observações")+ylab("Predições")+
scale_color_continuous(low="plum2",high="purple3",name="MSE")+
geom_abline(slope = 1,intercept = 0,color="gold")+
theme_hc(style="darkunica")
library(png)
library(grid)
library(ggplot2)
book_cov <- readPNG("../images/cover.png")
book_img <- rasterGrob(book_cov,interpolate = T)
ggplot()+annotation_custom(book_img,xmin = -Inf,xmax = Inf,ymin=-Inf,ymax=Inf)
library(png)
library(grid)
library(ggplot2)
book_cov <- readPNG("../images/cover.png")
book_img <- rasterGrob(book_cov,interpolate = T)
ggplot()+annotation_custom(book_img,xmin = -Inf,xmax = Inf,ymin=-Inf,ymax=Inf)
library(magrittr)
library(ggplot2)
set.seed(2600)
mark_ii <- function(x, y, eta, reps=1) {
# inicializa pesos randomicos de distribuicao normal
w1 <- rnorm(n = (dim(x)[2]+1)) %>% as.matrix # numero de pesos = numero de colunas em x + bias
w21 <- rnorm(2) %>% as.matrix
w22 <- rnorm(2) %>% as.matrix
ypreds <- rep(0,dim(x)[1]) # inicializa predicoes em 0
yerrors <- rep(0,dim(x)[1]) # inicializa predicoes em 0
for (j in 1:reps){
print(paste("This is training epoch:",j))
print(paste("Current weights:",w1,w21,w21))
# Processa as observacoes em x de forma aleatoria
for (i in sample(1:length(y),replace=F)) {
# predicao
ypred1 <- sum(w1 %*% c(as.numeric(x[i, ]),1))
ypred21 <- sum(w21 %*% as.numeric(ypred1))
ypred22 <- sum(w22 %*% as.numeric(ypred1))
out <- sum(ypred21,ypred22)
# update em w . Eta ja ajustado para 1/2*eta
delta_w22 <- eta * (-1) * (y[i] - (ypred21 + ypred22)) * ypred1 #d/dw22(targ - out)^2
delta_w21 <- eta * (-1) * (y[i] - (ypred21 + ypred22)) * ypred1 #d/dw21(targ - out)^2
#d/dw1(targ - out)^2 = 2(y[i] - (ypred21 + ypred22)) (-1) (sum(w21*w1*x[i,]) + sum(w21*w1*x[i,]))'
# d/dw1 (sum(w21*w1*x[i,]) + sum(w21*w1*x[i,])) =  (sum(w21*x[i,]) + sum(w21*x[i,]))
delta_w1 <- eta * (y[i] - (ypred21 + ypred22)) * -1 *
(sum(w21 %*% c(as.numeric(x[i,]),1)) + sum(w22 %*% c(as.numeric(x[i,]),1)))
#nota: x[i,] sera multiplicado como matriz (dot product)
w1 <- w1 - delta_w1
w21 <- w21 - delta_w21
w22 <- w22 - delta_w22
ypreds[i] <- out # salva predicao21 atual
yerrors[i] <- ypreds[i] - y[i]
}
print(paste("Mean squared error:", mean((yerrors)^2)))
}
return(ypreds)
}
mark_ii <- function(x, y, eta, reps=1) {
# inicializa pesos randomicos de distribuicao normal
w1 <- rnorm(n = (dim(x)[2]+1)) %>% as.matrix # numero de pesos = numero de colunas em x + bias
w21 <- rnorm(2) %>% as.matrix
w22 <- rnorm(2) %>% as.matrix
ypreds <- rep(0,dim(x)[1]) # inicializa predicoes em 0
yerrors <- rep(0,dim(x)[1]) # inicializa predicoes em 0
for (j in 1:reps){
print(paste("This is training epoch:",j))
print(paste("Current weights:",w1,w21,w21))
# Processa as observacoes em x de forma aleatoria
for (i in sample(1:length(y),replace=F)) {
# predicao
ypred1 <- sum(w1 %*% c(as.numeric(x[i, ]),1))
ypred21 <- sum(w21 %*% as.numeric(ypred1))
ypred22 <- sum(w22 %*% as.numeric(ypred1))
out <- sum(ypred21,ypred22)
# update em w . Eta ja ajustado para 1/2*eta
delta_w22 <- eta * (-1) * (y[i] - (ypred21 + ypred22)) * ypred1 #d/dw22(targ - out)^2
delta_w21 <- eta * (-1) * (y[i] - (ypred21 + ypred22)) * ypred1 #d/dw21(targ - out)^2
#d/dw1(targ - out)^2 = 2(y[i] - (ypred21 + ypred22)) (-1) (sum(w21*w1*x[i,]) + sum(w21*w1*x[i,]))'
# d/dw1 (sum(w21*w1*x[i,]) + sum(w21*w1*x[i,])) =  (sum(w21*x[i,]) + sum(w21*x[i,]))
delta_w1 <- eta * (y[i] - (ypred21 + ypred22)) * -1 *
(sum(w21 %*% c(as.numeric(x[i,]),1)) + sum(w22 %*% c(as.numeric(x[i,]),1)))
#nota: x[i,] sera multiplicado como matriz (dot product)
w1 <- w1 - delta_w1
w21 <- w21 - delta_w21
w22 <- w22 - delta_w22
ypreds[i] <- out # salva predicao21 atual
yerrors[i] <- ypreds[i] - y[i]
}
print(paste("Mean squared error:", mean((yerrors)^2)))
}
return(ypreds)
}
train_df <- iris[, c(1, 2, 3)]
names(train_df) <- c("s.len", "s.wid", "p.len")
head(train_df)
train_df[60:65,]
x_features <- train_df[, c(1, 2)]
y_target <- train_df[, 3]
# Convergencia boa
mark_ii_preds <- mark_ii(x = x_features,y = y_target,
eta=0.000001,reps = 40)
acc_data <- data.frame(y_preds=mark_ii_preds,
y_targs=y_target)
acc_data$errors <- y_target - mark_ii_preds
cor.test(acc_data$y_preds,acc_data$y_targs)
ggplot(acc_data,aes(y=y_preds,x=y_targs,color=errors))+
geom_point()+xlim(0,10)+ylim(0,10)+xlab("Observações")+ylab("Predições")+
scale_color_continuous(low="plum2",high="purple3",name="MSE")+
geom_abline(slope = 1,intercept = 0,color="gold")+
theme_hc(style="darkunica")
library(png)
library(grid)
library(ggplot2)
book_cov <- readPNG("../images/cover.png")
book_img <- rasterGrob(book_cov,interpolate = T)
ggplot()+annotation_custom(book_img,xmin = -Inf,xmax = Inf,ymin=-Inf,ymax=Inf)
library(magrittr)
library(ggplot2)
set.seed(2600)
mark_ii <- function(x, y, eta, reps=1) {
# inicializa pesos randomicos de distribuicao normal
w1 <- rnorm(n = (dim(x)[2]+1)) %>% as.matrix # numero de pesos = numero de colunas em x + bias
w21 <- rnorm(2) %>% as.matrix
w22 <- rnorm(2) %>% as.matrix
ypreds <- rep(0,dim(x)[1]) # inicializa predicoes em 0
yerrors <- rep(0,dim(x)[1]) # inicializa predicoes em 0
for (j in 1:reps){
print(paste("This is training epoch:",j))
print(paste("Current weights:",w1,w21,w21))
# Processa as observacoes em x de forma aleatoria
for (i in sample(1:length(y),replace=F)) {
# predicao
ypred1 <- sum(w1 %*% c(as.numeric(x[i, ]),1))
ypred21 <- sum(w21 %*% as.numeric(ypred1))
ypred22 <- sum(w22 %*% as.numeric(ypred1))
out <- sum(ypred21,ypred22)
# update em w . Eta ja ajustado para 1/2*eta
delta_w22 <- eta * (-1) * (y[i] - (ypred21 + ypred22)) * ypred1 #d/dw22(targ - out)^2
delta_w21 <- eta * (-1) * (y[i] - (ypred21 + ypred22)) * ypred1 #d/dw21(targ - out)^2
#d/dw1(targ - out)^2 = 2(y[i] - (ypred21 + ypred22)) (-1) (sum(w21*w1*x[i,]) + sum(w21*w1*x[i,]))'
# d/dw1 (sum(w21*w1*x[i,]) + sum(w21*w1*x[i,])) =  (sum(w21*x[i,]) + sum(w21*x[i,]))
delta_w1 <- eta * (y[i] - (ypred21 + ypred22)) * -1 *
(sum(w21 %*% c(as.numeric(x[i,]),1)) + sum(w22 %*% c(as.numeric(x[i,]),1)))
#nota: x[i,] sera multiplicado como matriz (dot product)
w1 <- w1 - delta_w1
w21 <- w21 - delta_w21
w22 <- w22 - delta_w22
ypreds[i] <- out # salva predicao21 atual
yerrors[i] <- ypreds[i] - y[i]
}
print(paste("Mean squared error:", mean((yerrors)^2)))
}
return(ypreds)
}
train_df <- iris[, c(1, 2, 3)]
names(train_df) <- c("s.len", "s.wid", "p.len")
head(train_df)
train_df[60:65,]
x_features <- train_df[, c(1, 2)]
y_target <- train_df[, 3]
y_target <- train_df[, 3]
# Convergencia boa
mark_ii_preds <- mark_ii(x = x_features,y = y_target,
eta=0.000001,reps = 40)
acc_data <- data.frame(y_preds=mark_ii_preds,
y_targs=y_target)
acc_data$errors <- y_target - mark_ii_preds
cor.test(acc_data$y_preds,acc_data$y_targs)
ggplot(acc_data,aes(y=y_preds,x=y_targs,color=errors))+
geom_point()+xlim(0,10)+ylim(0,10)+xlab("Observações")+ylab("Predições")+
scale_color_continuous(low="plum2",high="purple3",name="MSE")+
geom_abline(slope = 1,intercept = 0,color="gold")+
theme_hc(style="darkunica")
library(png)
library(grid)
library(ggplot2)
book_cov <- readPNG("../images/cover.png")
book_img <- rasterGrob(book_cov,interpolate = T)
ggplot()+annotation_custom(book_img,xmin = -Inf,xmax = Inf,ymin=-Inf,ymax=Inf)
felipe.c.argolo@protonmail.com
library(png)
library(grid)
library(ggplot2)
book_cov <- readPNG("../images/cover.png")
book_img <- rasterGrob(book_cov,interpolate = T)
ggplot()+annotation_custom(book_img,xmin = -Inf,xmax = Inf,ymin=-Inf,ymax=Inf)
library(magrittr)
library(ggplot2)
set.seed(2600)
mark_ii <- function(x, y, eta, reps=1) {
# inicializa pesos randomicos de distribuicao normal
w1 <- rnorm(n = (dim(x)[2]+1)) %>% as.matrix # numero de pesos = numero de colunas em x + bias
w21 <- rnorm(2) %>% as.matrix
w22 <- rnorm(2) %>% as.matrix
ypreds <- rep(0,dim(x)[1]) # inicializa predicoes em 0
yerrors <- rep(0,dim(x)[1]) # inicializa predicoes em 0
for (j in 1:reps){
print(paste("This is training epoch:",j))
print(paste("Current weights:",w1,w21,w21))
# Processa as observacoes em x de forma aleatoria
for (i in sample(1:length(y),replace=F)) {
# predicao
ypred1 <- sum(w1 %*% c(as.numeric(x[i, ]),1))
ypred21 <- sum(w21 %*% as.numeric(ypred1))
ypred22 <- sum(w22 %*% as.numeric(ypred1))
out <- sum(ypred21,ypred22)
# update em w . Eta ja ajustado para 1/2*eta
delta_w22 <- eta * (-1) * (y[i] - (ypred21 + ypred22)) * ypred1 #d/dw22(targ - out)^2
delta_w21 <- eta * (-1) * (y[i] - (ypred21 + ypred22)) * ypred1 #d/dw21(targ - out)^2
#d/dw1(targ - out)^2 = 2(y[i] - (ypred21 + ypred22)) (-1) (sum(w21*w1*x[i,]) + sum(w21*w1*x[i,]))'
# d/dw1 (sum(w21*w1*x[i,]) + sum(w21*w1*x[i,])) =  (sum(w21*x[i,]) + sum(w21*x[i,]))
delta_w1 <- eta * (y[i] - (ypred21 + ypred22)) * -1 *
(sum(w21 %*% c(as.numeric(x[i,]),1)) + sum(w22 %*% c(as.numeric(x[i,]),1)))
#nota: x[i,] sera multiplicado como matriz (dot product)
w1 <- w1 - delta_w1
w21 <- w21 - delta_w21
w22 <- w22 - delta_w22
ypreds[i] <- out # salva predicao21 atual
yerrors[i] <- ypreds[i] - y[i]
}
print(paste("Mean squared error:", mean((yerrors)^2)))
}
return(ypreds)
}
train_df <- iris[, c(1, 2, 3)]
names(train_df) <- c("s.len", "s.wid", "p.len")
head(train_df)
train_df[60:65,]
x_features <- train_df[, c(1, 2)]
y_target <- train_df[, 3]
y_target <- train_df[, 3]
# Convergencia boa
mark_ii_preds <- mark_ii(x = x_features,y = y_target,
eta=0.000001,reps = 40)
acc_data <- data.frame(y_preds=mark_ii_preds,
y_targs=y_target)
acc_data$errors <- y_target - mark_ii_preds
cor.test(acc_data$y_preds,acc_data$y_targs)
ggplot(acc_data,aes(y=y_preds,x=y_targs,color=errors))+
geom_point()+xlim(0,10)+ylim(0,10)+xlab("Observações")+ylab("Predições")+
scale_color_continuous(low="plum2",high="purple3",name="MSE")+
geom_abline(slope = 1,intercept = 0,color="gold")+
theme_hc(style="darkunica")
